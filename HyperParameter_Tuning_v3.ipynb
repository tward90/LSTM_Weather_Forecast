{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('WeatherML': conda)",
   "display_name": "Python 3.8.5 64-bit ('WeatherML': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fa0a90ea388e647b6806305426e0b9939b78a8fc99087e1c6cd6e6ae226ce148"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    import tensorflow as tf\n",
    "\n",
    "    df = pd.read_csv(\"differenced_data.csv\")\n",
    "    df.drop([\"Date\"], axis=1, inplace=True)\n",
    "\n",
    "    train_df = df[0:int(len(df)*.7)]\n",
    "    val_df = df[int(len(df)*.7):int(len(df)*.9)]\n",
    "    # test_df = df[int(len(df)*.9):]\n",
    "\n",
    "    # train_len = int(len(train_df)*.9)\n",
    "\n",
    "    # train_df = train_df[:train_len]\n",
    "    # val_df = train_df[train_len:]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_df)\n",
    "\n",
    "    train_scaled = scaler.transform(train_df)\n",
    "    val_scaled = scaler.transform(val_df)\n",
    "    # test_scaled = scaler.transform(test_df)\n",
    "\n",
    "    input_width = 6\n",
    "    label_width = 6\n",
    "    shift = 1\n",
    "    label_columns = None\n",
    "\n",
    "    column_indeces = {name: i for i, name in enumerate(train_df.columns)}\n",
    "    if label_columns is not None:\n",
    "        label_column_indeces = {name: i for i, name in enumerate(label_columns)}\n",
    "\n",
    "    window_size = input_width + shift\n",
    "\n",
    "    input_slice = slice(0, input_width)\n",
    "    input_indeces = np.arange(window_size)[input_slice]\n",
    "\n",
    "    label_start = window_size - label_width\n",
    "    label_slice = slice(label_start, None)\n",
    "    label_indeces =  np.arange(window_size)[label_slice]\n",
    "\n",
    "    def split_window(features):\n",
    "        inputs = features[:, input_slice, :]\n",
    "        labels = features[:, label_slice, :]\n",
    "        if label_columns:\n",
    "            labels = tf.stack([labels[:, :, column_indeces[name]] for name in label_columns], axis=1)\n",
    "\n",
    "        inputs.set_shape([None, input_width, None])\n",
    "        labels.set_shape([None, label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(data):\n",
    "        data = np.array(data, dtype=np.float64)\n",
    "        dset = timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "\n",
    "        dset = dset.map(split_window)\n",
    "        return dset\n",
    "\n",
    "    model_train = make_dataset(train_scaled)\n",
    "    model_val = make_dataset(val_scaled)\n",
    "\n",
    "    # # X_train = np.array(list(model_train.as_numpy_iterator()))[:,0]\n",
    "    # # X_train  = np.array(list(model_train.as_numpy_iterator()))[:,0].flatten()\n",
    "    # X_train = np.array(list(model_train.unbatch().as_numpy_iterator()))[:,0]\n",
    "    # # #594 batches with each batch containing an array of (32,6,12)\n",
    "\n",
    "    # # y_train = np.array(list(model_train.as_numpy_iterator()))[:,1]\n",
    "    # # y_train  = np.array(list(model_train.as_numpy_iterator()))[:,1].flatten()\n",
    "    # y_train = np.array(list(model_train.unbatch().as_numpy_iterator()))[:,1]\n",
    "\n",
    "    # # X_val = np.array(list(model_val.as_numpy_iterator()))[:,0]\n",
    "    # # X_val = np.array(list(model_val.as_numpy_iterator()))[:,0].flatten()\n",
    "    # X_val = np.array(list(model_val.unbatch().as_numpy_iterator()))[:,0]\n",
    "\n",
    "    # # y_val = np.array(list(model_val.as_numpy_iterator()))[:,1]\n",
    "    # # y_val = np.array(list(model_val.as_numpy_iterator()))[:,1].flatten()\n",
    "    # y_val = np.array(list(model_val.unbatch().as_numpy_iterator()))[:,1]\n",
    "\n",
    "    train_data = np.array(train_scaled)\n",
    "    val_data = np.array(val_scaled)\n",
    "\n",
    "    # train_len = int(len(train_data)*.9)\n",
    "\n",
    "    # train_data = train_data[:train_len]\n",
    "    # val_data = train_data[train_len:]\n",
    "\n",
    "\n",
    "    # end_index = train_data[:train_len]\n",
    "\n",
    "    # print(len(train_data))\n",
    "\n",
    "    train_data = timeseries_dataset_from_array(\n",
    "                                            data = train_data,\n",
    "                                            targets=None, \n",
    "                                            sequence_length=7, \n",
    "                                            shuffle=True, \n",
    "                                            batch_size=32)\n",
    "\n",
    "    val_data = timeseries_dataset_from_array(\n",
    "                                            data = val_data,\n",
    "                                            targets = None,\n",
    "                                            sequence_length = 7,\n",
    "                                            shuffle = True,\n",
    "                                            batch_size = 32)\n",
    "\n",
    "\n",
    "\n",
    "    # dataset.inputs.set_shape(None, 6, None)\n",
    "\n",
    "    train_dataset = train_data.map(split_window)\n",
    "    val_dataset = val_data.map(split_window)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(train_dataset, val_dataset):\n",
    "\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     from sklearn.metrics import mean_squared_error\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#     from tensorflow.keras.models import Sequential\n",
    "#     from tensorflow.keras.layers import Dense\n",
    "#     from tensorflow.keras.layers import LSTM\n",
    "#     import tensorflow as tf\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM({{choice([5, 10, 25, 50, 75, 100])}},\n",
    "#             return_sequences=True \n",
    "#            # ,input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "#         #    , batch_input_shape=(None, X_train.shape[1], X_train.shape[2])\n",
    "#             ))\n",
    "\n",
    "#     model.add(Dense({{choice([10, 20, 50, 100])}}, activation = 'relu'))\n",
    "#     model.add(Dense(12))\n",
    "#     model.compile(loss={{choice([\"mae\", \"mse\"])}}, \n",
    "#                 optimizer={{choice([\"adam\", \"sgd\", \"rmsprop\"])}}, \n",
    "#                 metrics=[\"mae\", \"mse\"]\n",
    "#                 )\n",
    "#     e_stop =  tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "\n",
    "#     # train_len = int(len(train_dataset)*.9)\n",
    "\n",
    "#     # result = model.fit(train_dataset[:train_len], epochs=25, batch_size=60, validation_data=train_dataset[:train_len], verbose=2, callbacks = [e_stop], shuffle=False)\n",
    "\n",
    "#     result = model.fit(train_dataset, epochs=25, batch_size=60, validation_data=val_dataset, verbose=2, callbacks = [e_stop], shuffle=False)\n",
    "\n",
    "#     # print(result.history)\n",
    "\n",
    "#     validation_loss = np.amin(result.history['val_loss'])\n",
    "#     print('Best Validation loss of epoch:', validation_loss)\n",
    "#     return {'loss': validation_loss, 'status': STATUS_OK, 'model':model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(train_dataset, val_dataset):\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    from tensorflow.keras.layers import LSTM\n",
    "    import tensorflow as tf\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM({{choice([5, 10, 25, 50, 75, 100])}},\n",
    "            return_sequences=True \n",
    "           # ,input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        #    , batch_input_shape=(None, X_train.shape[1], X_train.shape[2])\n",
    "            ))\n",
    "\n",
    "    model.add(Dense({{choice([10, 20, 50, 100])}}, activation = 'relu'))\n",
    "    model.add(Dense(12))\n",
    "    model.compile(loss={{choice([\"mae\", \"mse\"])}}, \n",
    "                optimizer={{choice([\"adam\", \"sgd\", \"rmsprop\"])}}, \n",
    "                metrics=[\"mae\", \"mse\"]\n",
    "                )\n",
    "    e_stop =  tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "\n",
    "    # train_len = int(len(train_dataset)*.9)\n",
    "\n",
    "    # result = model.fit(train_dataset[:train_len], epochs=25, batch_size=60, validation_data=train_dataset[:train_len], verbose=2, callbacks = [e_stop], shuffle=False)\n",
    "\n",
    "    result = model.fit(train_dataset, validation_data=val_dataset, epochs=25, batch_size=60,callbacks = [e_stop], verbose=2, shuffle=False)\n",
    "\n",
    "    # print(result.history)\n",
    "\n",
    "    validation_loss = np.amin(result.history['val_loss'])\n",
    "    print('Best Validation loss of epoch:', validation_loss)\n",
    "    test_model = histories[0][\"model\"]\n",
    "    plt.plot(model.history.history[\"loss\"], label=\"train\")\n",
    "    plt.plot(model.history.history[\"val_loss\"], label=\"test\")\n",
    "    plt.show()\n",
    "    return {'loss': validation_loss, 'status': STATUS_OK, 'model':model}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import mean_squared_error\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'LSTM': hp.choice('LSTM', [5, 10, 25, 50, 75, 100]),\n",
      "        'Dense': hp.choice('Dense', [10, 20, 50, 100]),\n",
      "        'loss': hp.choice('loss', [\"mae\", \"mse\"]),\n",
      "        'optimizer': hp.choice('optimizer', [\"adam\", \"sgd\", \"rmsprop\"]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: import pandas as pd\n",
      "   3: import numpy as np\n",
      "   4: from sklearn.preprocessing import StandardScaler\n",
      "   5: from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
      "   6: from tensorflow.keras.models import Sequential\n",
      "   7: from tensorflow.keras.layers import LSTM\n",
      "   8: from tensorflow.keras.layers import Dense\n",
      "   9: import tensorflow as tf\n",
      "  10: \n",
      "  11: df = pd.read_csv(\"differenced_data.csv\")\n",
      "  12: df.drop([\"Date\"], axis=1, inplace=True)\n",
      "  13: \n",
      "  14: train_df = df[0:int(len(df)*.7)]\n",
      "  15: val_df = df[int(len(df)*.7):int(len(df)*.9)]\n",
      "  16: # test_df = df[int(len(df)*.9):]\n",
      "  17: \n",
      "  18: # train_len = int(len(train_df)*.9)\n",
      "  19: \n",
      "  20: # train_df = train_df[:train_len]\n",
      "  21: # val_df = train_df[train_len:]\n",
      "  22: \n",
      "  23: scaler = StandardScaler()\n",
      "  24: scaler.fit(train_df)\n",
      "  25: \n",
      "  26: train_scaled = scaler.transform(train_df)\n",
      "  27: val_scaled = scaler.transform(val_df)\n",
      "  28: # test_scaled = scaler.transform(test_df)\n",
      "  29: \n",
      "  30: input_width = 6\n",
      "  31: label_width = 6\n",
      "  32: shift = 1\n",
      "  33: label_columns = None\n",
      "  34: \n",
      "  35: column_indeces = {name: i for i, name in enumerate(train_df.columns)}\n",
      "  36: if label_columns is not None:\n",
      "  37:     label_column_indeces = {name: i for i, name in enumerate(label_columns)}\n",
      "  38: \n",
      "  39: window_size = input_width + shift\n",
      "  40: \n",
      "  41: input_slice = slice(0, input_width)\n",
      "  42: input_indeces = np.arange(window_size)[input_slice]\n",
      "  43: \n",
      "  44: label_start = window_size - label_width\n",
      "  45: label_slice = slice(label_start, None)\n",
      "  46: label_indeces =  np.arange(window_size)[label_slice]\n",
      "  47: \n",
      "  48: def split_window(features):\n",
      "  49:     inputs = features[:, input_slice, :]\n",
      "  50:     labels = features[:, label_slice, :]\n",
      "  51:     if label_columns:\n",
      "  52:         labels = tf.stack([labels[:, :, column_indeces[name]] for name in label_columns], axis=1)\n",
      "  53: \n",
      "  54:     inputs.set_shape([None, input_width, None])\n",
      "  55:     labels.set_shape([None, label_width, None])\n",
      "  56: \n",
      "  57:     return inputs, labels\n",
      "  58: \n",
      "  59: def make_dataset(data):\n",
      "  60:     data = np.array(data, dtype=np.float64)\n",
      "  61:     dset = timeseries_dataset_from_array(\n",
      "  62:         data=data,\n",
      "  63:         targets=None,\n",
      "  64:         sequence_length=window_size,\n",
      "  65:         sequence_stride=1,\n",
      "  66:         shuffle=True,\n",
      "  67:         batch_size=32\n",
      "  68:     )\n",
      "  69: \n",
      "  70:     dset = dset.map(split_window)\n",
      "  71:     return dset\n",
      "  72: \n",
      "  73: model_train = make_dataset(train_scaled)\n",
      "  74: model_val = make_dataset(val_scaled)\n",
      "  75: \n",
      "  76: # # X_train = np.array(list(model_train.as_numpy_iterator()))[:,0]\n",
      "  77: # # X_train  = np.array(list(model_train.as_numpy_iterator()))[:,0].flatten()\n",
      "  78: # X_train = np.array(list(model_train.unbatch().as_numpy_iterator()))[:,0]\n",
      "  79: # # #594 batches with each batch containing an array of (32,6,12)\n",
      "  80: \n",
      "  81: # # y_train = np.array(list(model_train.as_numpy_iterator()))[:,1]\n",
      "  82: # # y_train  = np.array(list(model_train.as_numpy_iterator()))[:,1].flatten()\n",
      "  83: # y_train = np.array(list(model_train.unbatch().as_numpy_iterator()))[:,1]\n",
      "  84: \n",
      "  85: # # X_val = np.array(list(model_val.as_numpy_iterator()))[:,0]\n",
      "  86: # # X_val = np.array(list(model_val.as_numpy_iterator()))[:,0].flatten()\n",
      "  87: # X_val = np.array(list(model_val.unbatch().as_numpy_iterator()))[:,0]\n",
      "  88: \n",
      "  89: # # y_val = np.array(list(model_val.as_numpy_iterator()))[:,1]\n",
      "  90: # # y_val = np.array(list(model_val.as_numpy_iterator()))[:,1].flatten()\n",
      "  91: # y_val = np.array(list(model_val.unbatch().as_numpy_iterator()))[:,1]\n",
      "  92: \n",
      "  93: train_data = np.array(train_scaled)\n",
      "  94: val_data = np.array(val_scaled)\n",
      "  95: \n",
      "  96: # train_len = int(len(train_data)*.9)\n",
      "  97: \n",
      "  98: # train_data = train_data[:train_len]\n",
      "  99: # val_data = train_data[train_len:]\n",
      " 100: \n",
      " 101: \n",
      " 102: # end_index = train_data[:train_len]\n",
      " 103: \n",
      " 104: # print(len(train_data))\n",
      " 105: \n",
      " 106: train_data = timeseries_dataset_from_array(\n",
      " 107:                                         data = train_data,\n",
      " 108:                                         targets=None, \n",
      " 109:                                         sequence_length=7, \n",
      " 110:                                         shuffle=True, \n",
      " 111:                                         batch_size=32)\n",
      " 112: \n",
      " 113: val_data = timeseries_dataset_from_array(\n",
      " 114:                                         data = val_data,\n",
      " 115:                                         targets = None,\n",
      " 116:                                         sequence_length = 7,\n",
      " 117:                                         shuffle = True,\n",
      " 118:                                         batch_size = 32)\n",
      " 119: \n",
      " 120: \n",
      " 121: \n",
      " 122: # dataset.inputs.set_shape(None, 6, None)\n",
      " 123: \n",
      " 124: train_dataset = train_data.map(split_window)\n",
      " 125: val_dataset = val_data.map(split_window)\n",
      " 126: \n",
      " 127: \n",
      " 128: \n",
      " 129: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3: \n",
      "   4: \n",
      "   5: \n",
      "   6:     model = Sequential()\n",
      "   7:     model.add(LSTM(space['LSTM'],\n",
      "   8:             return_sequences=True \n",
      "   9:            # ,input_shape=(X_train.shape[1], X_train.shape[2]))\n",
      "  10:         #    , batch_input_shape=(None, X_train.shape[1], X_train.shape[2])\n",
      "  11:             ))\n",
      "  12: \n",
      "  13:     model.add(Dense(space['Dense'], activation = 'relu'))\n",
      "  14:     model.add(Dense(12))\n",
      "  15:     model.compile(loss=space['loss'], \n",
      "  16:                 optimizer=space['optimizer'], \n",
      "  17:                 metrics=[\"mae\", \"mse\"]\n",
      "  18:                 )\n",
      "  19:     e_stop =  tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
      "  20: \n",
      "  21:     # train_len = int(len(train_dataset)*.9)\n",
      "  22: \n",
      "  23:     # result = model.fit(train_dataset[:train_len], epochs=25, batch_size=60, validation_data=train_dataset[:train_len], verbose=2, callbacks = [e_stop], shuffle=False)\n",
      "  24: \n",
      "  25:     result = model.fit(train_dataset, validation_data=val_dataset, epochs=25, batch_size=60,callbacks = [e_stop], verbose=2, shuffle=False)\n",
      "  26: \n",
      "  27:     # print(result.history)\n",
      "  28: \n",
      "  29:     validation_loss = np.amin(result.history['val_loss'])\n",
      "  30:     print('Best Validation loss of epoch:', validation_loss)\n",
      "  31:     test_model = histories[0][\"model\"]\n",
      "  32:     plt.plot(model.history.history[\"loss\"], label=\"train\")\n",
      "  33:     plt.plot(model.history.history[\"val_loss\"], label=\"test\")\n",
      "  34:     plt.show()\n",
      "  35:     return {'loss': validation_loss, 'status': STATUS_OK, 'model':model}\n",
      "  36: \n",
      "Epoch 1/25\n",
      "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]WARNING:tensorflow:Layer lstm_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "594/594 - 5s - loss: 0.9044 - mae: 0.5695 - mse: 0.9044 - val_loss: 0.6104 - val_mae: 0.4914 - val_mse: 0.6104\n",
      "\n",
      "Epoch 2/25\n",
      "594/594 - 4s - loss: 0.8296 - mae: 0.5604 - mse: 0.8296 - val_loss: 0.5768 - val_mae: 0.4850 - val_mse: 0.5768\n",
      "\n",
      "Epoch 3/25\n",
      "594/594 - 4s - loss: 0.7996 - mae: 0.5546 - mse: 0.7996 - val_loss: 0.5590 - val_mae: 0.4763 - val_mse: 0.5590\n",
      "\n",
      "Epoch 4/25\n",
      "594/594 - 4s - loss: 0.7802 - mae: 0.5492 - mse: 0.7802 - val_loss: 0.5494 - val_mae: 0.4753 - val_mse: 0.5494\n",
      "\n",
      "Epoch 5/25\n",
      "594/594 - 4s - loss: 0.7650 - mae: 0.5451 - mse: 0.7650 - val_loss: 0.5418 - val_mae: 0.4723 - val_mse: 0.5418\n",
      "\n",
      "Epoch 6/25\n",
      "594/594 - 4s - loss: 0.7532 - mae: 0.5425 - mse: 0.7532 - val_loss: 0.5370 - val_mae: 0.4686 - val_mse: 0.5370\n",
      "\n",
      "Epoch 7/25\n",
      "594/594 - 4s - loss: 0.7448 - mae: 0.5406 - mse: 0.7448 - val_loss: 0.5363 - val_mae: 0.4680 - val_mse: 0.5363\n",
      "\n",
      "Epoch 8/25\n",
      "594/594 - 4s - loss: 0.7381 - mae: 0.5391 - mse: 0.7381 - val_loss: 0.5359 - val_mae: 0.4673 - val_mse: 0.5359\n",
      "\n",
      "Epoch 9/25\n",
      "594/594 - 4s - loss: 0.7329 - mae: 0.5374 - mse: 0.7329 - val_loss: 0.5359 - val_mae: 0.4653 - val_mse: 0.5359\n",
      "\n",
      "Epoch 10/25\n",
      "594/594 - 4s - loss: 0.7288 - mae: 0.5360 - mse: 0.7288 - val_loss: 0.5342 - val_mae: 0.4670 - val_mse: 0.5342\n",
      "\n",
      "Epoch 11/25\n",
      "594/594 - 4s - loss: 0.7256 - mae: 0.5350 - mse: 0.7256 - val_loss: 0.5311 - val_mae: 0.4638 - val_mse: 0.5311\n",
      "\n",
      "Epoch 12/25\n",
      "594/594 - 4s - loss: 0.7230 - mae: 0.5342 - mse: 0.7230 - val_loss: 0.5289 - val_mae: 0.4656 - val_mse: 0.5289\n",
      "\n",
      "Epoch 13/25\n",
      "594/594 - 4s - loss: 0.7205 - mae: 0.5335 - mse: 0.7205 - val_loss: 0.5259 - val_mae: 0.4613 - val_mse: 0.5259\n",
      "\n",
      "Epoch 14/25\n",
      "594/594 - 4s - loss: 0.7186 - mae: 0.5327 - mse: 0.7186 - val_loss: 0.5244 - val_mae: 0.4611 - val_mse: 0.5244\n",
      "\n",
      "Epoch 15/25\n",
      "594/594 - 3s - loss: 0.7168 - mae: 0.5322 - mse: 0.7168 - val_loss: 0.5238 - val_mae: 0.4608 - val_mse: 0.5238\n",
      "\n",
      "Epoch 16/25\n",
      "594/594 - 4s - loss: 0.7153 - mae: 0.5317 - mse: 0.7153 - val_loss: 0.5246 - val_mae: 0.4618 - val_mse: 0.5246\n",
      "\n",
      "Epoch 17/25\n",
      "594/594 - 4s - loss: 0.7139 - mae: 0.5313 - mse: 0.7139 - val_loss: 0.5216 - val_mae: 0.4599 - val_mse: 0.5216\n",
      "\n",
      "Epoch 18/25\n",
      "594/594 - 5s - loss: 0.7127 - mae: 0.5309 - mse: 0.7127 - val_loss: 0.5220 - val_mae: 0.4597 - val_mse: 0.5220\n",
      "\n",
      "Epoch 19/25\n",
      "594/594 - 4s - loss: 0.7116 - mae: 0.5304 - mse: 0.7116 - val_loss: 0.5229 - val_mae: 0.4603 - val_mse: 0.5229\n",
      "\n",
      "Epoch 20/25\n",
      "594/594 - 4s - loss: 0.7106 - mae: 0.5301 - mse: 0.7106 - val_loss: 0.5214 - val_mae: 0.4588 - val_mse: 0.5214\n",
      "\n",
      "Epoch 21/25\n",
      "594/594 - 4s - loss: 0.7097 - mae: 0.5297 - mse: 0.7097 - val_loss: 0.5214 - val_mae: 0.4563 - val_mse: 0.5214\n",
      "\n",
      "Epoch 22/25\n",
      "594/594 - 4s - loss: 0.7089 - mae: 0.5294 - mse: 0.7089 - val_loss: 0.5213 - val_mae: 0.4582 - val_mse: 0.5213\n",
      "\n",
      "Epoch 23/25\n",
      "594/594 - 5s - loss: 0.7082 - mae: 0.5292 - mse: 0.7082 - val_loss: 0.5214 - val_mae: 0.4585 - val_mse: 0.5214\n",
      "\n",
      "Epoch 24/25\n",
      "594/594 - 4s - loss: 0.7076 - mae: 0.5290 - mse: 0.7076 - val_loss: 0.5202 - val_mae: 0.4567 - val_mse: 0.5202\n",
      "\n",
      "Epoch 25/25\n",
      "594/594 - 4s - loss: 0.7069 - mae: 0.5288 - mse: 0.7069 - val_loss: 0.5224 - val_mae: 0.4613 - val_mse: 0.5224\n",
      "\n",
      "Best Validation loss of epoch:\n",
      "0.5202122926712036\n",
      "  0%|          | 0/5 [01:47<?, ?trial/s, best loss=?]job exception: name 'histories' is not defined\n",
      "\n",
      "  0%|          | 0/5 [01:47<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'histories' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-30881d6c392b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     best_run, best_model = optim.minimize(model=create_model,\n\u001b[0m\u001b[1;32m     11\u001b[0m                                             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                             \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/WeatherML/lib/python3.8/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mreturn_space\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mpair\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbest\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[0msearch\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \"\"\"\n\u001b[0;32m---> 59\u001b[0;31m     best_run, space = base_minimizer(model=model,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                      \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                                      \u001b[0mfunctions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/WeatherML/lib/python3.8/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     return (\n\u001b[0;32m--> 133\u001b[0;31m         fmin(keras_fmin_fnct,\n\u001b[0m\u001b[1;32m    134\u001b[0m              \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m              \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/WeatherML/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mallow_trials_fmin\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fmin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         return trials.fmin(\n\u001b[0m\u001b[1;32m    508\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/WeatherML/lib/python3.8/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfmin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m         return fmin(\n\u001b[0m\u001b[1;32m    683\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/WeatherML/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/WeatherML/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/WeatherML/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/WeatherML/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/WeatherML/lib/python3.8/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    905\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m             )\n\u001b[0;32m--> 907\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Rice/Projects/Project_3/Project03_Team01/temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[0;34m(space)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'histories' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    from hyperas import optim\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from hyperopt import Trials, tpe\n",
    "    from tensorflow.keras.layers import LSTM\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    import tensorflow as tf\n",
    "\n",
    "    best_run, best_model = optim.minimize(model=create_model,\n",
    "                                            data=data,\n",
    "                                            algo=tpe.suggest,\n",
    "                                            max_evals=5,\n",
    "                                            trials=Trials(),\n",
    "                                            notebook_name = 'HyperParameter_Tuning_v3')\n",
    "                                            \n",
    "    train_dataset, val_dataset = data()\n",
    "    print('Evaluation of best performing model:')\n",
    "    print(best_model.evaluate(val_dataset))\n",
    "    print(\"Best Performing Model Hyper-Parameters:\")\n",
    "    print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'Sequential' object is not subscriptable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-200e7eca675d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Sequential' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test_model = best_model[0][\"model\"]\n",
    "plt.plot(best_model.history[\"loss\"], label=\"train\")\n",
    "plt.plot(best_model.history[\"val_loss\"], label=\"test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'Sequential' object is not subscriptable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1b787aafcc9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Sequential' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "best_model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}